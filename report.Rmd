---
title: "Glovo Data Science Interview"
author: "Sergi Vilardell"
date: "7 June 2018"
output: 
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
runtime: shiny
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F,  warn.conflicts = F)
library(tidyverse)
library(data.table)
library(corrplot)
library(viridis)
library(GGally)
library(magrittr)
library(MASS)
library(plotly)
library(rsconnect)
library(randomForest)
```

## Task 1: Exploratory Analysis

Let us describe the data given a little bit and take a peek at its values and ranges:

```{r}
# Read data
lifetime.df <- fread("Courier_lifetime_data.csv", sep = ",", header= TRUE)
weekly.df <- fread("Courier_weekly_data.csv", sep = ",", header= TRUE)

summary(weekly.df)
summary(lifetime.df)
```


### Lifetime Data

Let us begin with the lifetime data.
```{r}
glimpse(lifetime.df)
```


Here we have a column for the id of each `courier`, a categorical feature in `feature_1` with values `a,b,c,d`, and a numerical feature in `feature_2`. Note that in `feature_2` there are `NA`, therefore we should compute how many instances with missing values there are:

```{r}
# Add column to account for the NAs
# T == NA, F != NA
lifetime.df <- lifetime.df %>% 
	mutate( is_na = ifelse(is.na(feature_2), T, F))

# Check how many NAs we have
lifetime.df %>%  
	count(is_na)
```

About $12\%$ of the instances in `feature_2` are `NA` so they should not be ignored. It is interesting to see the number of instances for each group in `feature_1` and also the distribution of `NA` in aforementioned groups.

```{r echo=FALSE}
ggplot(data = lifetime.df, aes( x = feature_1, fill = is_na))+
	geom_bar(colour = "black")+
	ggtitle("Lifetime's feature_1 barplot")+
	theme_minimal()
```

Here we can see that the distribution of instances is not even among the groups in `feature_1`. Nevertheless, the proportion of `NA` values for each group seems to be similar.

```{r}
# Counting NAs
lifetime.df %>% 
	group_by(feature_1) %>% 
	summarise(sum(is_na)/n())
```

Except ifrom `c` all other groups have similar `NA` proportion, which maybe due to the low number of instances in `c`. 



Here we show a quick plot to viasualize `feature_2`:

```{r echo=FALSE}
plot(lifetime.df$feature_2, main = "Overview of Lifetime's feature 2")
```

All values lie within the same range except some outliers: the three instances with value above 900.

```{r}
# Filtering big numbers
lifetime.df %>% 
	drop_na() %>% 
	filter(feature_2 > 200)
```
 And the negative numbers:
 
```{r}
# Filtering negatives numbers
lifetime.df %>% 
	drop_na() %>% 
	filter(feature_2 < 0)
```
 
 They are outliers not because their value differ from the mean, but because they are the only negative numbers and so they should be treated with caution depending on what `feature_2` means. Without knowing what this data is representing we can only stand out its deviation from the regular behaviour. Furthermore, both outliers belong to either group `a` or `b`. 
 
Let us see the distribution of `feature_2` without the outliers:
```{r echo=FALSE}
filtered.data <- lifetime.df %>% 
	drop_na() %>% 
	filter(feature_2 %in% -1:75)
	
ggplot(data = filtered.data, aes(x = feature_2))+
	geom_histogram(binwidth = 1)+
	theme_minimal()+
	ggtitle("Histogram of feature_2")
```


Also, separated by group:
```{r echo=FALSE}
ggplot(data = filtered.data, aes(x = feature_2))+
	geom_histogram(binwidth = 1)+
	facet_wrap(~feature_1)+
	theme_minimal()+
	ggtitle("Histrograms of feature_2 by feature_1")
```

In order to fill the `NA` instances we will use bootstrap separately for each group in `feature_1`. In this way each instance is represented in the same way as the others from each group. This is a reasonable solution to fill `NA` values without knowing the meaning of the variable, since it still preserves the distribution of the values in `feature_2`.

```{r include=FALSE}
lifetime.df <- lifetime.df %>% 
  mutate( is_na = ifelse(is.na(feature_2), T, F))

table.NA <- table(lifetime.df$is_na, lifetime.df$feature_1)


list.NA <- list()
 
group = c("a", "b", "c", "d")

for( i in 1:4){
beh <- lifetime.df %>%
    filter(feature_1 == group[i], is_na == F) 
  
list.NA[[i]] <- sample(beh$feature_2, table.NA[,group[i]][2], replace = T)
}

# GUARRO

a.df <-  lifetime.df%>% 
  filter(feature_1 == "a") 

a.df$feature_2[is.na(a.df$feature_2)] <- list.NA[[1]][1:sum(a.df$is_na)]

b.df <-  lifetime.df%>% 
  filter(feature_1 == "b") 

b.df$feature_2[is.na(b.df$feature_2)] <- list.NA[[2]][1:sum(b.df$is_na)]

c.df <-  lifetime.df%>% 
  filter(feature_1 == "c") 

c.df$feature_2[is.na(c.df$feature_2)] <- list.NA[[3]][1:sum(c.df$is_na)]

d.df <-  lifetime.df%>% 
  filter(feature_1 == "d") 

d.df$feature_2[is.na(d.df$feature_2)] <- list.NA[[4]][1:sum(d.df$is_na)]


lifetime.df <- rbind(a.df, b.df, c.df, d.df)

```

```{r echo=FALSE}
filtered.data <- lifetime.df %>% 
	drop_na() %>% 
	filter(feature_2 %in% -1:75)
	
ggplot(data = filtered.data, aes(x = feature_2))+
	geom_histogram(binwidth = 1)+
  	theme_minimal()+
  	ggtitle("Histogram of feature_2 without NAs")
```



### Weekly Data

```{r}
glimpse(weekly.df)
```

Here, in `weekly_data` there is a record for the worked weeks of some of the couriers. There is the `courier` column with the courier id, the column `week` for the worked week, and the rest are numerical features from 1 to 17. Let us explore those variables by plotting their distributions and the correlations among them


```{r echo=FALSE}

weekly.df <<- weekly.df
inputPanel(
  selectInput("features", label = "Select features:",
              choices = colnames(weekly.df), 
  						selected = c("feature_1", "feature_3", "feature_4", "feature_5", "feature_7"), 
  						multiple = T)

)
  
renderPlot({

ggpairs(weekly.df, 
		columns = input$features, aes(alpha = 0.2))+
		theme_minimal()+
		ggtitle("Correlation and Distribution plots for Weekly data")

},height = 600, width = 800 
)

```

It is interesting to look in detail at some correlations. The first that stands out the most is the correlation between `feature_4` and `feature_5` as they have a correlation of $-1$. We can easily see that the features follow `feature_4 + feature_5 = 1`, meaning that we can get rid of either one because they contain redudant information. 

Another interesting one is `feature_3` which seems drawn from a theoretical distribution, we will explore deeper there later on. This `feature_3` is highly correlated to `feature_2` and `feature_11`.`feature_14` seems to be correlated to `feature_1`, and aside from these examples the other features do not have a relevant correlation to one another. `feature_8` presents a behaviour that also seems drawn from the combination of theoretical probability distributions.

We can also explore the distribution of the data by grouping `weekly_data` with `feature_1` of `lifetime_data` to see if we can gather some more information.

```{r echo=FALSE, fig.width=10}
# Add lifetime feature_1 into weekly 
ids <-  match(weekly.df[["courier"]], lifetime.df[["courier"]])
b <- as.vector(lifetime.df[ids,2])
weekly.df$feature_18 <- b

weekly.df <<- weekly.df
inputPanel(
  selectInput("feat", label = "Select features:",
              choices = colnames(weekly.df), 
  						selected = c("feature_3", "feature_4", "feature_5", "feature_7"), 
  						multiple = T)

)
  
renderPlot({

ggpairs(weekly.df, 
		columns = input$feat,  
		aes(colour = weekly.df$feature_18, 
			alpha = 0.5))+
		theme_minimal()+
		ggtitle("Correlation and Distribution plots for Weekly data by feature_18")

},height = 600, width = 800 
)


```

Our assumption that `feature_2` from `lifetime_data` may be able to distinguish some couriers from the others seems not to hold. Correlations between variables are very similar even among groups. The ony group that stands out is `c`. The correlation when computed with the instances of group `c` are very high among variables, but this could be due to the low sample size of group `c` so it should not be trusted. 


### Feature 3

Let us analyse `feature_3` more deeply:

```{r echo=FALSE}

emp <- weekly.df$feature_3
emp.df <- weekly.df%>% 
  dplyr::select(feature_3)

ggplot(data = emp.df, aes (x = feature_3 ))+
  geom_histogram(binwidth = 1)+
  theme_minimal()+
  ggtitle("HIstogram of feature_3")

```


This looks like the data is generated from a theoretical distribution. Let us look at some skewed probability distributions and see how they fit into our empirical distribution. We do so by estimating the parameters that best fit our empirical data compared to different theoretical distributions. We then generate theoretical data with those parameters, and compare the empirical and theoretical distributions:

```{r}
# Poisson
fit_poisson <- fitdistr(emp, "poisson")
theo <- rpois(length(emp),
			  lambda = fit_poisson$estimate[["lambda"]])
```

```{r echo=FALSE}
qqplot(emp,theo, 
	   main = "QQ plot with Possion distribution", 
	   ylab = "Theoretical Quantiles",
	   xlab = "Empirical Quantiles")
```

The Poission distribution does not seem to match our distribution.

```{r}
# Gamma
fit_gamma <- fitdistr(emp, "gamma")
theo <- rgamma(length(emp), 
			   shape = fit_gamma$estimate[["shape"]], 
			   rate = fit_gamma$estimate[["rate"]])
```

```{r echo=FALSE}
qqplot(emp,theo, 
	   main = "QQ plot with Gamma distribution", 
	   ylab = "Theoretical Quantiles",
	   xlab = "Empirical Quantiles")
```


The Gamma distribution seems to fit better, but let us look into another distribution:

```{r}
# Weibull
fit_weibull <- fitdistr(emp, "weibull")
theo <- rweibull(length(emp), 
				 shape = fit_weibull$estimate[["shape"]], 
				 scale = fit_weibull$estimate[["scale"]])
```

```{r echo=FALSE}
qqplot(emp,theo, 
	   main = "QQ plot with Gamma distribution", 
	   ylab = "Theoretical Quantiles",
	   xlab = "Empirical Quantiles")
```



The Weibull distribution seems to fit quite well. It looks quite like straight line, but let us make it sure. Let us build a Kolmogorov-Smirnov test. First we perform a Kolmogorov-Smirnov test with all the data from the empirical distribution:

```{r}
# Kolmogorov-Smirnov
e0 <- ks.test(emp,
              "pweibull",
			  shape = fit_weibull$estimate[["shape"]],
              scale = fit_weibull$estimate[["scale"]]
)$statistic
e0
```

With Kolmogorov distance $D = 0.02$ this is a good indicator of similarity. But in order to not reject the hypothesis that our empirical distribution is a Weibull distribution, we need to perform the same test multiple times with different samples of our data. Each time we perform the test the statistic, in this case the Kolmogorov distance, is stored. When all tests are done we plot the histogram of the statistic. If the histogram follows a Kolmogorov distribution, then the hypothesis that our distribution is a Weibull is passed. Let us perform the test:

```{r eval=FALSE}
# Kolmogorov-Smirnov sampling test
estimates <- c()
for(i in 1:1e5){
  estimates[i] <- ks.test(sample(emp, size = 1e2, replace = TRUE),
                          "pweibull", shape = fit_weibull$estimate[["shape"]],
                          scale = fit_weibull$estimate[["scale"]]
  )$statistic
}


```

```{r echo=FALSE}
estimates.df <- read.csv("estimates.csv", sep = ",", header= TRUE)
ggplot(data = estimates.df, aes(x = estimates))+
  geom_histogram(binwidth = 0.003)+
  theme_minimal()+
  ggtitle("Histogram of the KS-test statistic")
```


We can see that the histogram of the statistic follows a Kolmogorov distribution quite well. Moreover we can compute the p-value of this test by looking at the position of the first estimate for the Kolmogorov distances that we computed $D = 0.02$ in the cumulative distribution function of the estimates: 

```{r}
# Evaluation of the test
z <- ecdf(estimates.df$estimates)
1-z(e0)
```
 With p-value $1$ we can say that the hypothesis that our empirical distribution is a Weibull passes. So by the tests performed in the qqplot and the Kolmogorov-Smirnov test we can say with great confidence that `feature_3` is generated from a Weibull distribution. Nevertheless, the data from `feature_3` is discrete, and the Weibull is a continuous distribution. It looks as if the data is generated by a Weibull and then it has been truncated.
 
```{r}
# Truncating a theoretical Weibull distribution
theo.df <- data.frame(theo = floor(rweibull(length(emp), 
					  shape = fit_weibull$estimate[["shape"]],
					  scale = fit_weibull$estimate[["scale"]])
					  ))
```

```{r echo=FALSE}

theo.plot <- ggplot(data = theo.df, aes(x = theo))+
  geom_histogram(binwidth = 1)+
  theme_minimal()

emp.plot <- ggplot(data = emp.df, aes (x = feature_3 ))+
  geom_histogram(binwidth = 1)+
  theme_minimal()

ggmatrix(list(theo.plot, emp.plot), nrow = 2, ncol = 1, yAxisLabels= c("Theoretical","Empirical"), xlab = "asd")+
  ggtitle("Comparison of the theoretical and empirical histograms")
```


They look very similar. This should be enough evidence to claim that `feature_3` is indeed a truncated Weibull distribution.


### Churn

Let us explore how many weeks the couriers work and if it has any impact on the data they show.

```{r echo=FALSE}
weekly.df %>%
  group_by(courier) %>%
  summarise(n = n()) %>%
  ggplot() +
  geom_bar(aes(x = as.factor(n), y  = ..count../sum(..count..)*100, fill = (..count..))) +
  theme_minimal() +
  scale_fill_viridis(option = "D") +
  theme(legend.position = "NONE") +
  xlab("Number of Weeks worked") +
  ylab("(%)")+
  ggtitle("Barplot of total worked weeks")
```


We can see that approximately $15\%$ of the couriers only work 1 week, let us analyse what happens with them
```{r}
# Churn
churn_df <- weekly.df %>%
  group_by(courier) %>%
  mutate(churn = (n() == 1)) %>%
  filter(churn == TRUE)

nochurn_df <-  weekly.df %>%
  group_by(courier) %>%
  mutate(churn = (n() == 1)) %>%
  filter(churn == FALSE)

churn_df <- churn_df %>%
  sample_n(size = floor(nrow(nochurn_df)/nrow(churn_df)), replace = TRUE)

churned <- rbind(churn_df, nochurn_df)
```

We create the variable `churn` to label the couriers with `1` if they only work one week, and `0` otherwise.

```{r echo=FALSE}

churned <<- churned
inputPanel(
  selectInput("feature", label = "Select features:",
              choices = colnames(churned), 
  						selected = c("feature_1","feature_3", "feature_5", "feature_7"), 
  						multiple = T)

)
  
renderPlot(
	{ggpairs(churned, 
			columns = input$feature,  
			aes(colour = churn, alpha = 0.5))+
			theme_minimal()+
			ggtitle("Correlation and distribution plots for Weekly data by churn")},

	height = 600, width = 800 
)

```

We can see how the couriers behave differently, whether they churn or not in `feature_3` and the other features correlated to it. Now depending on the meaning of the variables one could infer what makes a courier work more or less than the others. In the other features we do not see much difference between the couriers, so it may be that the couriers behave similarly overall except from the features mentioned before, which could give us a hint on why they only have worked one week.

### Label Data

```{r include=FALSE}
weekly.df <- fread("Courier_weekly_data.csv", sep = ",", header= TRUE)
```


Here, we label the data according to the guidance given. It is interesting now to generate once more the correlation and distribution plots grouped by label. In order to give each courier one instance of data as it would be fed into the model, we collapse all instances of each courier with their mean. Also we create an additional variable named `worked_weeks` which may be relevant to classify the couriers.
```{r}
# Labeling the data

colnames(lifetime.df) <- c("courier", "feature_18", "feature_19")
total.df <- merge(weekly.df, lifetime.df, all = FALSE, by = "courier")
ids <- unique(total.df$courier)
total.df$target <- NA
train.df <- total.df %>% data.frame()
weeks.of.interest <- c(9,10,11)
courier <- train.df[["courier"]]
week <- train.df[["week"]]
for(i in ids){
  if(sum(week[which(courier == i)] %in% weeks.of.interest) == 3){
    train.df[train.df$courier == i,"target"] <- 0
  }else{
    train.df[train.df$courier == i,"target"] <- 1
  }
}

train.df <- train.df[,-22]
train.df %<>% 
  group_by(courier) %>% 
  filter(!(week %in% c(8,9,10,11))) %>% 
  mutate(worked_weeks = as.numeric(n())) %>%
  summarise_all("mean") 


```


```{r echo=FALSE}

train.df <<- fread("train.csv", sep = ",", header= TRUE)
inputPanel(
  selectInput("featur", label = "Select features:",
              choices = colnames(train.df)[-c(1,2)], 
  						selected = c("feature_1", "feature_2","feature_3", "feature_4","worked_weeks"), 
  						multiple = T)

)
  
renderPlot({

ggpairs(train.df, columns = input$featur,  aes(colour = as.factor(target), alpha = 0.3))+theme_minimal()

},height = 600, width = 800 
)
```

Here, we can see that the feature that seems to separate best the couriers is `feature_3`, and all the variables highly correlated with it. Also, `worked_weeks` seems to separate them quite well, which may give us a hint on what the label means. It seems that the label separates hard workers from light workers, or any other indicator that seems to be related to that.



## Task 2: Predictive Algorithm

For constructing the predictive model we choose the random forest. The advantadge of random forests is that it is fairly easy to tune and does not need a validation test set. Every time a tree is generated a sample is taken, in this case $70\%$ of the training set, and validated with the rest of the dataset. In this way, the out of bag error (`OOB`) gives us the same error as if we divided the dataset into the classic train/test. This fits our problem very well as we have low number of instances and we need as much as possible to train the model. The random forest will also tell us which variables are more relevant to classification, giving us insight of our data from which we can make decisions on. 

```{r}
# Random Forest
set.seed(666)
train.df <- fread("train.csv", sep = ",", header= TRUE)
train.df$target <- as.factor(train.df$target)
train.df$feature_18 <- as.factor(train.df$feature_18)
train.df %<>%
	dplyr::select(-week, -courier)

rf_model <- randomForest(formula = target ~ ., 
                         data = train.df, 
                         mtry = 4, 
                         ntree = 8000, 
						 sampsize = floor(0.7*nrow(train.df)),
                         nodesize=15
                         )
```

We have chosen to tune the parameters manually, given that we have a low number of instances for one of the classes. If we were to use a fine hyperparameter tuning on our model there is a high chance of committing overfitting even when tuning for cross validation.


## Task 3: Evaluating the Model

In this section we present the model with all the variables and instances:

```{r}
# Variable Importance Plot
varImpPlot(rf_model)
```

The most relevant feature for the model is `feature_3`. Next, we have `feature_11` and `feature_2` which is no surprise given that they are highly correlated to `feature_3`. Then we have `worked_weeks`. We want to dissert for a bit here to talk about why we did not fill the missing weeks for the couriers. It seems that couriers missing weeks of work is a relevant feature for distinguishing the label and it has something to do with the behaviour of the couriers. As we can see in the plot of correlations between features, `worked_weeks` is correlated with relevant features like `feature_3`. We could have filled the missing week instances for couriers using bootstrap for instance, to preserve the distribution of the features. But it does not seem appropriate given that couriers express different behaviours depending of how many weeks they have worked, as we can see in `worked_weeks` feature and also in churn plots. Therefore, in the end we would be doing either two things: filling the `NA` with statistically representative data without still being able to distinguish them because we do not take into account the different behaviour that couriers exhibit, or we could take the behaviour into account and introduce bias in our model, as we can only represent their behaviour with the data we are provided, which is not necessarely representative of the real one.

Here, we show the performance of the model:

```{r, echo = F}
# Performance
rf_model
```


The model is capable of predicting the `1` with  an error of only $3\%$. Nevertheless, due to the small sample size of instances with target `0` the model predicts them with an error of $74\%$. There was an attempt to create new variables for the model, instead of collapsing the weekly data of the courier into their mean we took the minimum and maximum value of their worked weeks. It turned out that the model did not improve as the variance explained by those new features is the same as the first ones. 

We can explore another model where the data is balanced in terms of the class to predict. Let us equalize the number of instances for each class by taking a sample of the instances of `1`  equal to the number of instances of `0`.

```{r}
# Random Forest with reduced data
zero.df <- train.df %>% 
  filter(target == 0) 

one.df <- train.df %>% 
  filter(target == 1) %>% 
  sample_n(nrow(zero.df))

train.sampled <- rbind(zero.df, one.df)
rf_model_alt <- randomForest(formula = target ~ ., 
                         data = train.sampled, 
                         mtry = 4, 
                         ntree = 8000,
					     sampsize = floor(0.7*nrow(train.sampled)),
                         nodesize=15
                         )
```

```{r echo=F}
varImpPlot(rf_model_alt)
```

```{r echo=F}
print(rf_model_alt)
```

With this model we have a higher `OOB` error of about $25\%$ so it seems that overall the model is worse. But, if we look closely at the classification errors we can see that the classification error for `0` has reduced from $74\%$ to $22\%$, while the error for `1` has increased from $3\%$ to $30\%$. This does not mean that the model is necessarely worse, it depends on the purpose of the model. For instance, imagine that we want to classify patients from a hospital in the categories `ill` and `not ill`. For a doctor it is crucial to know with high accuracy if a patient is ill and will not tolerate high error in this classification, but will not mind if a healthy patient is classified as `ill`. In this case the first model is useful if label `1` represents ill patients, as it has low classification error. Another situation could be that we need to predict credit default for clients in a bank. It would happen that we have low data for the ones defaulting the credit, as is the case of our `0`. A model that does not classify them well is of no interest to the bank, even if we are able to classify very well non default clients. For the bank it is more useful a model like the second one, where the overall classification error is higher, but the label of interest has lower misclasification. In our case, we do not know which is the intention of this classification as we do not know exactly what each label means, or the variables involved. We present both models that could be suited for both cases.