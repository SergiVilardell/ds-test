---
title: "Glovo Data Science Interview"
author: "Sergi Vilardell"
date: "7 June 2018"
output: 
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
runtime: shiny
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F,  warn.conflicts = F)
library(tidyverse)
library(data.table)
library(corrplot)
library(viridis)
library(GGally)
library(magrittr)
```

## Task 1: Exploratory Analysis

Let us describe the data given a little bit:



```{r}
#Read data
lifetime.df <- fread("Courier_lifetime_data.csv", sep = ",", header= TRUE)
weekly.df <- fread("Courier_weekly_data.csv", sep = ",", header= TRUE)

summary(weekly.df)
summary(lifetime.df)
```


### Lifetime Data

Let us begin with the lifetime data.
```{r}
glimpse(lifetime.df)
```


Here we have a column for the id of each `courier`, a categorical feature in `feature_1` with values `a,b,c,d`, and a numerical feature in `feature_2`. Note that in `feature_2` there are `NA`, therefore we should compute how many instances there are with missing values:

```{r}
#Add column to account for the NAs
# T == NA, F != NA
lifetime.df <- lifetime.df %>% 
	mutate( is_na = ifelse(is.na(feature_2), T, F))

#Check how many NAs we have
lifetime.df %>%  
	count(is_na)
```

So about $12\%$ of the instances in `feature_2` are `NA` so they should not be ignored. It is interesting to see the number of instances fo each group in `feature_1` and also the distribution of `NA` in said groups.

```{r}
ggplot(data = lifetime.df, aes( x = feature_1, fill = is_na))+
	geom_bar(colour = "black")+
	theme_minimal()
  
```

Here we can see that the distribution of instances is not even among the groups in `feature_1`. Nevertheless the proportion of `NA` values for each group seems to be similar.

```{r}
lifetime.df %>% 
	group_by(feature_1) %>% 
	summarise(sum(is_na)/n())
```

Except in `c` all other groups have similar `NA` proportion, which maybe due to the low number of instances in `c`. 



Quick plot to viasualize `feature_2`:

```{r}
plot(lifetime.df$feature_2)
```

All values lie within the same range except some outliers: the three instances with value above 900.

```{r}
lifetime.df %>% 
	drop_na() %>% 
	filter(feature_2 > 200)
```
 And the negative numbers:
 
```{r}
lifetime.df %>% 
	drop_na() %>% 
	filter(feature_2 < 0)
```
 
 They are outliers not because their value differ from the mean, but because they are the only negative numbers and so they should be treated with caution depending on what `feature_2` means. Without knowing what this data is representing we can only stand out its deviation from the regular behaviour. Furthermore both outliers belong to either group `a` or `b`. 
 
Let us see the distribution of `feature_2` without the outliers:
```{r}
filtered.data <- lifetime.df %>% 
	drop_na() %>% 
	filter(feature_2 %in% -1:75)
	
ggplot(data = filtered.data, aes(x = feature_2))+
	geom_histogram(binwidth = 1)+
  theme_minimal()+
  ggtitle("Histogram of feature_2")
```


Also separated by group:
```{r}
ggplot(data = filtered.data, aes(x = feature_2))+
	geom_histogram(binwidth = 1)+
	facet_wrap(~feature_1)+
	theme_minimal()+
  ggtitle("Histrograms of feature_2 by feature_1")
```

In order to fill the `NA` instances we will use bootstrap separately for each group in `feature_1`.

### Weekly Data

```{r, fig.width= 10, fig.width= 10, echo = T}
library(GGally)
library(plotly)
library(rsconnect)
library(data.table)

#Add lifetime feature_1 into weekly 
ids <-  match(weekly.df[["courier"]], lifetime.df[["courier"]])
b <- as.vector(lifetime.df[ids,2])
weekly.df$feature_18 <- b

weekly.df <<- weekly.df
inputPanel(
  selectInput("features", label = "Number of features:",
              choices = c("courier", "week", 
              			"feature_1", 
              			"feature_2", 
              			"feature_3",
              			"feature_4", 
              			"feature_5",
              			"feature_6", 
              			"feature_7", 
              			"feature_8", 
              			"feature_9", 
              			"feature_10", 
              			"feature_11", 
              			"feature_12",
              			"feature_13",
              			"feature_14",
              			"feature_15",
              			"feature_16",
              			"feature_17"), 
  						selected = c("feature_4", "feature_5","feature_6", "feature_7"), 
  						multiple = T)

)
  
renderPlot({

ggpairs(weekly.df, columns = input$features,  aes(colour = weekly.df$feature_18, alpha = 0.5))+theme_minimal()

},height = 600, width = 800 
)


```


```{r}
weekly.df %>%
  group_by(courier) %>%
  summarise(n = n()) %>%
  ggplot() +
  geom_bar(aes(x = as.factor(n), y  = ..count../sum(..count..)*100, fill = (..count..))) +
  theme_minimal() +
  scale_fill_viridis(option = "D") +
  theme(legend.position = "NONE") +
  xlab("Number of Weeks worked") +
  ylab("(%)")+
  ggtitle("Barplot of total worked weeks")
```


$15\%$ of the couriers only work 1 week, let us analyse what happens with them
```{r}
#Churnition
churn_df <- weekly.df %>%
  group_by(courier) %>%
  mutate(churn = (n() == 1)) %>%
  filter(churn == TRUE)

nochurn_df <-  weekly.df %>%
  group_by(courier) %>%
  mutate(churn = (n() == 1)) %>%
  filter(churn == FALSE)

churn_df <- churn_df %>%
  sample_n(size = floor(nrow(nochurn_df)/nrow(churn_df)), replace = TRUE)

churned <- rbind(churn_df, nochurn_df)
```

Plot with churn
```{r}

churned <<- churned
inputPanel(
  selectInput("feature", label = "Number of features:",
              choices = c("courier", "week", 
              			"feature_1", 
              			"feature_2", 
              			"feature_3",
              			"feature_4", 
              			"feature_5",
              			"feature_6", 
              			"feature_7", 
              			"feature_8", 
              			"feature_9", 
              			"feature_10", 
              			"feature_11", 
              			"feature_12",
              			"feature_13",
              			"feature_14",
              			"feature_15",
              			"feature_16",
              			"feature_17"), 
  						selected = c("feature_4", "feature_5","feature_6", "feature_7"), 
  						multiple = T)

)
  
renderPlot({

ggpairs(churned, columns = input$feature,  aes(colour = churn, alpha = 0.5))+theme_minimal()

},height = 600, width = 800 
)

```


### Feature 3

Let us analyse `feature_3` more deeply:

```{r}
library(MASS)
emp <- weekly.df$feature_3
emp.df <- weekly.df%>% 
  dplyr::select(feature_3)

ggplot(data = emp.df, aes (x = feature_3 ))+
  geom_histogram(binwidth = 1)+
  theme_minimal()+
  ggtitle("HIstogram of feature_3")

```


This looks like the data is generated from a theoretical distribution. Let us look at some skewed probability distributions and see how they fit into our empirical distribution. We do so by estimating the parameters that best fit our empirical data compared to different theoretical distributions. We then generate theoretical data with those parameters, and compare the empirical and theoretical distributions:

```{r}
# Poisson
fit_poisson <- fitdistr(emp, "poisson")
theo <- rpois(length(emp), lambda = fit_poisson$estimate[["lambda"]])
qqplot(emp,theo)
```

The Poission distribution does not seem to match our distribution.

```{r}
# Gamma
fit_gamma <- fitdistr(emp, "gamma")
theo <- rgamma(length(emp), shape = fit_gamma$estimate[["shape"]], rate = fit_gamma$estimate[["rate"]])
qqplot(emp,theo)
```

The gamma distribution seems to fit better, but let us look into another distribution:

```{r}
# Weibull
fit_weibull <- fitdistr(emp, "weibull")
theo <- rweibull(length(emp), shape = fit_weibull$estimate[["shape"]], scale = fit_weibull$estimate[["scale"]])
qqplot(emp,theo)
```

The Weibull distribution seems to fit quite well. It looks quite like straight line, but let us make it sure. Let us build a Kolmogorov-Smirnov test. First we perform a Kolmogorov-Smirnov test with all the data from the empirical distribution:

```{r}
e0 <- ks.test(emp,
              "pweibull", shape = fit_weibull$estimate[["shape"]],
              scale = fit_weibull$estimate[["scale"]]
)$statistic
e0
```

With Kolmogorov distance $D = 0.02$ this is a good indicator of similarity. But in order to not reject the hypothesis that our empirical distribution is a Weibull distribution, we need to perform the same test multiple times with different samples of our data. Each time we perform the test the statistic is stored. When all tests are done we plot the histogram of the statistic. If the histogram follows a Kolmogorov distribution, we cannot reject the hypothesis that our empirical distribution is not a Weibull distribution. Let us perform the test:

```{r}

estimates <- c()
for(i in 1:1e4){
  estimates[i] <- ks.test(sample(emp, size = 1e2, replace = TRUE),
                          "pweibull", shape = fit_weibull$estimate[["shape"]],
                          scale = fit_weibull$estimate[["scale"]]
  )$statistic
}

estimates.df <- data.frame(estimates)
ggplot(data = estimates.df, aes(x = estimates))+
  geom_histogram(binwidth = 0.003)+
  theme_minimal()+
  ggtitle("Histogram of the KS-test statistic")
```

We can see that the histogram of the statistic follows a Kolmogorov distribution quite well. Moreover we can compute the p-value of this test by looking at the position of the first estimate for the Kolmogorov distances that we computed $D = 0.02$ in the cumulative distribution function of the estimates: 

```{r}
z <- ecdf(estimates)
1-z(e0)
```
 With p-value $1$ we cannot reject the hypothesis that our empirical distribution is not a Weibull. So by the tests performed in the qqplot and the Kolmogorov-Smirnov test we can say with great confidence that `feature_3` is generated from a Weibull distribution. Nevertheless the data from `feature_3` is discrete, and the Weibull is a continuous distribution. It looks as if the data is generated by a Weibull and then has been truncated.
 
```{r}
theo.df <- data.frame(theo = floor(rweibull(length(emp), shape = fit_weibull$estimate[["shape"]], scale = fit_weibull$estimate[["scale"]])))

theo.plot <- ggplot(data = theo.df, aes(x = theo))+
  geom_histogram(binwidth = 1)+
  theme_minimal()

emp.plot <- ggplot(data = emp.df, aes (x = feature_3 ))+
  geom_histogram(binwidth = 1)+
  theme_minimal()

ggmatrix(list(theo.plot, emp.plot), nrow = 2, ncol = 1, yAxisLabels= c("Theoretical","Empirical"), xlab = "asd")+
  ggtitle("Comparison of the theoretical and empirical histograms")
```

They look very similar. This should be enough evidence to claim that `feature_3` is indeed a truncated Weibull distribution.

### Label Data

```{r}
# Train Model -------------------------------------------------------------

colnames(lifetime.df) <- c("courier", "feature_18", "feature_19")
total.df <- merge(weekly.df, lifetime.df, all = FALSE, by = "courier")


ids <- unique(total.df$courier)
total.df$target <- NA
train.df <- total.df %>% data.frame()
weeks.of.interest <- c(9,10,11)
courier <- train.df[["courier"]]
week <- train.df[["week"]]
for(i in ids){
  if(sum(week[which(courier == i)] %in% weeks.of.interest) == 3){
    train.df[train.df$courier == i,"target"] <- 0
  }else{
    train.df[train.df$courier == i,"target"] <- 1
  }
}


train.df <- train.df[, -c(20,21)] 
train.df %<>% 
  group_by(courier) %>% 
  filter(!(week %in% c(8,9,10,11))) %>% 
  summarise_all("mean") 
```


```{r}


train.df <<- train.df 
inputPanel(
  selectInput("featur", label = "Number of features:",
              choices = c("courier", "week", 
              			"feature_1", 
              			"feature_2", 
              			"feature_3",
              			"feature_4", 
              			"feature_5",
              			"feature_6", 
              			"feature_7", 
              			"feature_8", 
              			"feature_9", 
              			"feature_10", 
              			"feature_11", 
              			"feature_12",
              			"feature_13",
              			"feature_14",
              			"feature_15",
              			"feature_16",
              			"feature_17"), 
  						selected = c("feature_1", "feature_2","feature_3", "feature_4"), 
  						multiple = T)

)
  
renderPlot({

ggpairs(train.df, columns = input$featur,  aes(colour = as.factor(target), alpha = 0.3))+theme_minimal()

},height = 600, width = 800 
)
```


## Task 2: Predictive Algorithm

Lorem fistrum benemeritaar mamaar jarl quietooor sexuarl caballo blanco caballo negroorl. Mamaar no puedor está la cosa muy malar la caidita jarl. Me cago en tus muelas está la cosa muy malar está la cosa muy malar sexuarl sexuarl hasta luego Lucas sexuarl se calle ustée caballo blanco caballo negroorl la caidita. La caidita amatomaa por la gloria de mi madre te voy a borrar el cerito te voy a borrar el cerito. A wan hasta luego Lucas va usté muy cargadoo condemor no te digo trigo por no llamarte Rodrigor sexuarl de la pradera fistro tiene musho peligro. Me cago en tus muelas pecador sexuarl apetecan benemeritaar llevame al sircoo diodeno.


## Task 3: Evaluating the Model

Lorem fistrum benemeritaar mamaar jarl quietooor sexuarl caballo blanco caballo negroorl. Mamaar no puedor está la cosa muy malar la caidita jarl. Me cago en tus muelas está la cosa muy malar está la cosa muy malar sexuarl sexuarl hasta luego Lucas sexuarl se calle ustée caballo blanco caballo negroorl la caidita. La caidita amatomaa por la gloria de mi madre te voy a borrar el cerito te voy a borrar el cerito. A wan hasta luego Lucas va usté muy cargadoo condemor no te digo trigo por no llamarte Rodrigor sexuarl de la pradera fistro tiene musho peligro. Me cago en tus muelas pecador sexuarl apetecan benemeritaar llevame al sircoo diodeno.

